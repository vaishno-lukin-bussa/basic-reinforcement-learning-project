# ğŸ‹ï¸â€â™‚ï¸ Q-Learning Agent for CartPole

This project implements a **Q-Learning agent** to solve the classic **CartPole-v1** environment from [Gymnasium](https://gymnasium.farama.org/).  
The agent learns to balance a pole on a moving cart by interacting with the environment and updating a **Q-table**.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ play.py                   #Run a trained agent and watch it play
â”œâ”€â”€ q_learning_agent.py       #Q-learning Agent implementation
â”œâ”€â”€ q_table.npy               #Saved Q-table after training
â”œâ”€â”€ requirements.txt          #Dependencies
â”œâ”€â”€ train.py                  #Script to train the agent
â”œâ”€â”€ training_performance.png  #Reward curve over training
```

---

## âš™ï¸ How It Works

### 1. Environment
The agent is trained in the **CartPole-v1** environment, where:
- The cart can move **left** or **right**.
- The goal is to prevent the pole from falling by applying the right forces.

### 2. State Discretization
Since CartPole has **continuous states**, we discretize them into buckets:
- Cart Position
- Cart Velocity
- Pole Angle
- Pole Angular Velocity  

This converts the state space into a finite grid for a **Q-table** lookup.

### 3. Q-Learning Algorithm
The agent updates its **Q-values** using the Bellman equation:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \big]
\]

- **s**: current state  
- **a**: action taken  
- **r**: reward received  
- **s'**: next state  
- **Î± (alpha)**: learning rate  
- **Î³ (gamma)**: discount factor  

### 4. Exploration vs Exploitation
The agent uses an **epsilon-greedy policy**:
- With probability **Îµ**, it explores (random action).  
- With probability **1-Îµ**, it exploits the best action from the Q-table.  
- Îµ decays over time â†’ more exploitation as training progresses.

---

## ğŸ¯ Reward Calculation

In **CartPole-v1**, the reward is provided **by the environment**:
- **+1 reward for every time step the pole remains upright**  
- If the pole falls or the cart goes out of bounds â†’ **episode ends**

So, the **total reward for an episode** equals the **duration (steps survived)**.

For example:
- If the pole stays balanced for 250 steps â†’ **Reward = 250**  
- If it falls at 50 steps â†’ **Reward = 50**

The Q-learning agent uses this feedback to improve its policy.

---

## ğŸš€ Training the Agent

Run:
```bash
python train.py
```

This will:
- Train the agent for **20,000 episodes**
- Save the learned **q_table.npy**
- Generate a **performance plot (training_performance.png)**

---

## ğŸ® Watching the Agent Play

Once trained, watch the agent balance the pole:
```bash
python play.py
```

The agent will:
- Load the trained **q_table.npy**
- Play one episode using the learned policy (no random actions)
- Display **Steps survived** directly in the Pygame window

---

## ğŸ“Š Results

Training performance is logged and visualized:  
- **Blue line** â†’ reward per episode  
- **Red line** â†’ moving average (trend of learning progress)  

Example plot (`training_performance.png`):  
![Training Performance](training_performance.png)

---

## ğŸ“¦ Requirements


### Create and Activate Virtual Environment (Windows PowerShell)

Before installing dependencies, create and activate a virtual environment:

```powershell
python -m venv venv
venv\Scripts\activate
```

Then install dependencies:

```bash
pip install -r requirements.txt
```

Dependencies:
- `gymnasium[classic-control]`
- `numpy`
- `matplotlib`
- `pygame`

---

## ğŸ“ Summary

- Implements **Q-learning** from scratch  
- Solves **CartPole-v1** using **state discretization**  
- Stores learned policy in **Q-table**  
- Visualizes training performance  
- Displays **steps survived in real-time** on the Pygame window  

---
